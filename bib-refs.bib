@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{rombach2022highresolutionimagesynthesislatent,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10752}, 
}

@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}
@inproceedings{wavenet,title	= {WaveNet: A Generative Model for Raw Audio},author	= {Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},year	= {2016},URL	= {https://arxiv.org/abs/1609.03499},booktitle	= {Arxiv}}

@article{Suzuki_2022,
   title={A survey of multimodal deep generative models},
   volume={36},
   ISSN={1568-5535},
   url={http://dx.doi.org/10.1080/01691864.2022.2035253},
   DOI={10.1080/01691864.2022.2035253},
   number={5–6},
   journal={Advanced Robotics},
   publisher={Informa UK Limited},
   author={Suzuki, Masahiro and Matsuo, Yutaka},
   year={2022},
   month=feb, pages={261–278} }
@misc{goodfellow2014generativeadversarialnetworks,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.2661}, 
}

@misc{nowozin2016fgantraininggenerativeneural,
      title={f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}, 
      author={Sebastian Nowozin and Botond Cseke and Ryota Tomioka},
      year={2016},
      eprint={1606.00709},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1606.00709}, 
}

@article{Nguyen_2010,
   title={Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization},
   volume={56},
   ISSN={1557-9654},
   url={http://dx.doi.org/10.1109/TIT.2010.2068870},
   DOI={10.1109/tit.2010.2068870},
   number={11},
   journal={IEEE Transactions on Information Theory},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
   year={2010},
   month=nov, pages={5847–5861} }
@inproceedings{10.1145/3283254.3283282,
author = {Zhang, Zhaoyu and Li, Mengyan and Yu, Jun},
title = {On the convergence and mode collapse of GAN},
year = {2018},
isbn = {9781450360623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283254.3283282},
doi = {10.1145/3283254.3283282},
abstract = {Generative adversarial network (GAN) is a powerful generative model. However, it suffers from several problems, such as convergence instability and mode collapse. To overcome these drawbacks, this paper presents a novel architecture of GAN, which consists of one generator and two different discriminators. With the fact that GAN is the analogy of a minimax game, the proposed architecture is as follows. The generator (G) aims to produce realistic-looking samples to fool both of two discriminators. The first discriminator (D1) rewards high scores for samples from the data distribution, while the second one (D2) favors samples from the generator conversely. Specifically, the ResBlock and minibatch discrimination (MD) architectures are adopted in D1 to improve the diversity of the samples. The leaky rectified linear unit (Leaky ReLU) and batch normalization (BN) are replaced by the scaled exponential linear unit (SELU) in D2 to alleviate the convergence problem. A new loss function that minimizes the KL divergence is designed to better optimize the model. Extensive experiments on CIFAR-10/100 datasets demonstrate that the proposed method can effectively solve the problems of convergence and mode collapse.},
booktitle = {SIGGRAPH Asia 2018 Technical Briefs},
articleno = {21},
numpages = {4},
keywords = {mode collapse, convergence, GAN},
location = {Tokyo, Japan},
series = {SA '18}
}
@misc{kingma2022autoencodingvariationalbayes,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1312.6114}, 
}
@misc{gimenez2022unifiedfdivergenceframeworkgeneralizing,
      title={A Unified f-divergence Framework Generalizing VAE and GAN}, 
      author={Jaime Roquero Gimenez and James Zou},
      year={2022},
      eprint={2205.05214},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.05214}, 
}

@misc{song2021scorebasedgenerativemodelingstochastic,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.13456}, 
}
@misc{sun2024simpleeffectivepruningapproach,
      title={A Simple and Effective Pruning Approach for Large Language Models}, 
      author={Mingjie Sun and Zhuang Liu and Anna Bair and J. Zico Kolter},
      year={2024},
      eprint={2306.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.11695}, 
}
@misc{verbockhaven2024growingtinynetworksspotting,
      title={Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally}, 
      author={Manon Verbockhaven and Sylvain Chevallier and Guillaume Charpiat},
      year={2024},
      eprint={2405.19816},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.19816}, 
}

@misc{zhuang2020comprehensivesurveytransferlearning,
      title={A Comprehensive Survey on Transfer Learning}, 
      author={Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
      year={2020},
      eprint={1911.02685},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.02685}, 
}
@article{JMLR:v6:hyvarinen05a,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695-709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	number = {7},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	month = jul,
	year = {2011},
	pages = {1661--1674},
}

@article{sugiyama_density_nodate,
	title = {Density {Ratio} {Estimation}: {A} {Comprehensive} {Review}},
	abstract = {Density ratio estimation has attracted a great deal of attention in the statistics and machine learning communities since it can be used for solving various statistical data processing tasks such as non-stationarity adaptation, two-sample test, outlier detection, independence test, feature selection/extraction, independent component analysis, causal inference, and conditional probability estimation. When estimating the density ratio, it is preferable to avoid estimating densities since density estimation is known to be a hard problem. In this paper, we give a comprehensive review of density ratio estimation methods based on moment matching, probabilistic classiﬁcation, and ratio matching.},
	language = {en},
	author = {Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
}
@misc{song2020generativemodelingestimatinggradients,
      title={Generative Modeling by Estimating Gradients of the Data Distribution}, 
      author={Yang Song and Stefano Ermon},
      year={2020},
      eprint={1907.05600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.05600}, 
}
@misc{ho2020denoisingdiffusionprobabilisticmodels,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.11239}, 
}
@misc{sohldickstein2015deepunsupervisedlearningusing,
      title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics}, 
      author={Jascha Sohl-Dickstein and Eric A. Weiss and Niru Maheswaranathan and Surya Ganguli},
      year={2015},
      eprint={1503.03585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1503.03585}, 
}
@book{oksendal2003stochastic,
  title={Stochastic differential equations},
  author={{\O}ksendal, Bernt and {\O}ksendal, Bernt},
  year={2003},
  publisher={Springer}
}
@article{ANDERSON1982313,
title = {Reverse-time diffusion equation models},
journal = {Stochastic Processes and their Applications},
volume = {12},
number = {3},
pages = {313-326},
year = {1982},
issn = {0304-4149},
doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
author = {Brian D.O. Anderson},
abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.}
}

@misc{kpj1992numerical,
  title={Numerical Solutions of Stochastic Differential Equations},
  author={KPj, PE KLOEDEN and PLATEN, E},
  year={1992},
  publisher={Springer-Verlag}
}

@misc{karras2022elucidatingdesignspacediffusionbased,
      title={Elucidating the Design Space of Diffusion-Based Generative Models}, 
      author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
      year={2022},
      eprint={2206.00364},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.00364}, 
}
@misc{uehara_generative_2016,
	title = {Generative {Adversarial} {Nets} from a {Density} {Ratio} {Estimation} {Perspective}},
	url = {http://arxiv.org/abs/1610.02920},
	abstract = {Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Uehara, Masatoshi and Sato, Issei and Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
	month = nov,
	year = {2016},
	note = {arXiv:1610.02920 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{xu_restart_2023,
	title = {Restart {Sampling} for {Improving} {Generative} {Processes}},
	url = {http://arxiv.org/abs/2306.14878},
	abstract = {Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet {\textbackslash}64 {\textbackslash}textbackslashtimes 64{\textbackslash}. In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION {\textbackslash}512 {\textbackslash}textbackslashtimes 512{\textbackslash}. Code is available at https://github.com/Newbeeer/diffusion\_restart\_sampling},
	urldate = {2023-12-23},
	publisher = {arXiv},
	author = {Xu, Yilun and Deng, Mingyang and Cheng, Xiang and Tian, Yonglong and Liu, Ziming and Jaakkola, Tommi},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Computation, Statistics - Machine Learning},
	annote = {arXiv:2306.14878 [cs, stat]},
}
@misc{lai2023fpdiffusionimprovingscorebaseddiffusion,
      title={FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation}, 
      author={Chieh-Hsin Lai and Yuhta Takida and Naoki Murata and Toshimitsu Uesaka and Yuki Mitsufuji and Stefano Ermon},
      year={2023},
      eprint={2210.04296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.04296}, 
}
@misc{chao2023investigatingconservativepropertyscorebased,
      title={On Investigating the Conservative Property of Score-Based Generative Models}, 
      author={Chen-Hao Chao and Wei-Fang Sun and Bo-Wun Cheng and Chun-Yi Lee},
      year={2023},
      eprint={2209.12753},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.12753}, 
}

@techreport{minka2005divergence,
  title={Divergence measures and message passing},
  author={Minka, Tom and others},
  year={2005},
  institution={Technical report, Microsoft Research}
}
@article{verine2024precision,
  title={Precision-recall divergence optimization for generative modeling with GANs and normalizing flows},
  author={Verine, Alexandre and Negrevergne, Benjamin and Pydi, Muni Sreenivas and Chevaleyre, Yann},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{Nguyen_2009,
   title={On surrogate loss functions and f-divergences},
   volume={37},
   ISSN={0090-5364},
   url={http://dx.doi.org/10.1214/08-AOS595},
   DOI={10.1214/08-aos595},
   number={2},
   journal={The Annals of Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
   year={2009},
   month=apr }
@article{Kingma_2019,
   title={An Introduction to Variational Autoencoders},
   volume={12},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000056},
   DOI={10.1561/2200000056},
   number={4},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Kingma, Diederik P. and Welling, Max},
   year={2019},
   pages={307–392} }

@misc{nowozin2016fgantraininggenerativeneural,
      title={f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}, 
      author={Sebastian Nowozin and Botond Cseke and Ryota Tomioka},
      year={2016},
      eprint={1606.00709},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1606.00709}, 
}

@misc{dhariwal2021diffusionmodelsbeatgans,
      title={Diffusion Models Beat GANs on Image Synthesis}, 
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.05233}, 
}

@misc{lucic2019highfidelityimagegenerationfewer,
      title={High-Fidelity Image Generation With Fewer Labels}, 
      author={Mario Lucic and Michael Tschannen and Marvin Ritter and Xiaohua Zhai and Olivier Bachem and Sylvain Gelly},
      year={2019},
      eprint={1903.02271},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1903.02271}, 
}

@misc{ho2022classifierfreediffusionguidance,
      title={Classifier-Free Diffusion Guidance}, 
      author={Jonathan Ho and Tim Salimans},
      year={2022},
      eprint={2207.12598},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.12598}, 
}
@misc{rombach2022highresolutionimagesynthesislatent,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10752}, 
}


@inproceedings{lai_fp-diffusion_2023,
	title = {Fp-diffusion: {Improving} score-based diffusion models by enforcing the underlying score fokker-planck equation},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lai, Chieh-Hsin and Takida, Yuhta and Murata, Naoki and Uesaka, Toshimitsu and Mitsufuji, Yuki and Ermon, Stefano},
	year = {2023},
	pages = {18365--18398},
}
@inproceedings{
salimans2021should,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}
@misc{chao2023investigatingconservativepropertyscorebased,
      title={On Investigating the Conservative Property of Score-Based Generative Models}, 
      author={Chen-Hao Chao and Wei-Fang Sun and Bo-Wun Cheng and Chun-Yi Lee},
      year={2023},
      eprint={2209.12753},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.12753}, 
}
@misc{saremi2018deepenergyestimatornetworks,
      title={Deep Energy Estimator Networks}, 
      author={Saeed Saremi and Arash Mehrjou and Bernhard Schölkopf and Aapo Hyvärinen},
      year={2018},
      eprint={1805.08306},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1805.08306}, 
}

@InCollection{vonN51,
  Title                    = {Various techniques used in connection with random digits},
  Author                   = {John {von Neumann}},
  Booktitle                = {Monte Carlo Method},
  Publisher                = {National Bureau of Standards Applied Mathematics Series, 12},
  Year                     = {1951},

  Address                  = {Washington, D.C.: U.S. Government Printing Office},
  Editor                   = {A.S. Householder and G.E. Forsythe and H.H. Germond},
  Pages                    = {36--38}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@misc{azadi2019discriminatorrejectionsampling,
      title={Discriminator Rejection Sampling}, 
      author={Samaneh Azadi and Catherine Olsson and Trevor Darrell and Ian Goodfellow and Augustus Odena},
      year={2019},
      eprint={1810.06758},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1810.06758}, 
}

@misc{brock2019largescalegantraining,
      title={Large Scale GAN Training for High Fidelity Natural Image Synthesis}, 
      author={Andrew Brock and Jeff Donahue and Karen Simonyan},
      year={2019},
      eprint={1809.11096},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.11096}, 
}

@inproceedings{liu2015faceattributes,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@misc{verine2024optimalbudgetedrejectionsampling,
      title={Optimal Budgeted Rejection Sampling for Generative Models}, 
      author={Alexandre Verine and Muni Sreenivas Pydi and Benjamin Negrevergne and Yann Chevaleyre},
      year={2024},
      eprint={2311.00460},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.00460}, 
}
@misc{kim2023refininggenerativeprocessdiscriminator,
      title={Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models}, 
      author={Dongjun Kim and Yeongmin Kim and Se Jung Kwon and Wanmo Kang and Il-Chul Moon},
      year={2023},
      eprint={2211.17091},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2211.17091}, 
}
@misc{kim2022maximumlikelihoodtrainingimplicit,
      title={Maximum Likelihood Training of Implicit Nonlinear Diffusion Models}, 
      author={Dongjun Kim and Byeonghu Na and Se Jung Kwon and Dongsoo Lee and Wanmo Kang and Il-Chul Moon},
      year={2022},
      eprint={2205.13699},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.13699}, 
}

@misc{goodfellow2014generativeadversarialnetworks,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.2661}, 
}@inproceedings{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fréchet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2023-01-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year = {2017},
}

@inproceedings{kynkaanniemi_improved_2019,
	title = {Improved {Precision} and {Recall} {Metric} for {Assessing} {Generative} {Models}},
	abstract = {The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.},
	urldate = {2021-05-26},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019), {Vancouver}, {Canada}.},
	author = {Kynkäänniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	month = oct,
	year = {2019},
	note = {arXiv: 1904.06991},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}


@misc{xu_restart_2023,
	title = {Restart {Sampling} for {Improving} {Generative} {Processes}},
	url = {http://arxiv.org/abs/2306.14878},
	abstract = {Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet {\textbackslash}64 {\textbackslash}textbackslashtimes 64{\textbackslash}. In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION {\textbackslash}512 {\textbackslash}textbackslashtimes 512{\textbackslash}. Code is available at https://github.com/Newbeeer/diffusion\_restart\_sampling},
	urldate = {2023-12-23},
	publisher = {arXiv},
	author = {Xu, Yilun and Deng, Mingyang and Cheng, Xiang and Tian, Yonglong and Liu, Ziming and Jaakkola, Tommi},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Computation, Statistics - Machine Learning},
	annote = {arXiv:2306.14878 [cs, stat]},
}
@article{Dalalyan_2019,
   title={User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient},
   volume={129},
   ISSN={0304-4149},
   url={http://dx.doi.org/10.1016/j.spa.2019.02.016},
   DOI={10.1016/j.spa.2019.02.016},
   number={12},
   journal={Stochastic Processes and their Applications},
   publisher={Elsevier BV},
   author={Dalalyan, Arnak S. and Karagulyan, Avetik},
   year={2019},
   month=dec, pages={5278–5311} }

@misc{mnih2013playingatarideepreinforcement,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5602}, 
}

@misc{gupta2019relaypolicylearningsolving,
      title={Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning}, 
      author={Abhishek Gupta and Vikash Kumar and Corey Lynch and Sergey Levine and Karol Hausman},
      year={2019},
      eprint={1910.11956},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.11956}, 
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}


@inproceedings{agarwal_f-policy_2023,
	title = {f-{Policy} {Gradients}: {A} {General} {Framework} for {Goal}-{Conditioned} {RL} using f-{Divergences}},
	url = {https://openreview.net/forum?id=EhhPtGsVAv},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Agarwal, Siddhant and Durugkar, Ishan and Stone, Peter and Zhang, Amy},
	year = {2023},
	file = {Agarwal et al. - 2023 - f-Policy Gradients A General Framework for Goal-C.pdf:C\:\\Users\\inane\\Zotero\\storage\\63AZSICG\\Agarwal et al. - 2023 - f-Policy Gradients A General Framework for Goal-C.pdf:application/pdf},
}

@inproceedings{NIPS1999_464d828b,
 author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
 volume = {12},
 year = {1999}
}
