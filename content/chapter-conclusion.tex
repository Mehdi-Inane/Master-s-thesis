% !TEX root = ../thesis-example.tex
%
\chapter{Conclusion}\label{sec:conclusion}
In this work, we were interested in proposing methods for refining pre-trained diffusion models. Error sources in diffusion mainly stem from two aspects : approximation error, related to the training process, and sampling error, rekated to the discretization of the backward SDE.
In section \ref{sec:dg}, we presented discriminator guidance (DG) a method that is used to correct the approximation error. We improved upon the existing method by deriving an optimal training objective, and demonstrated our results on a two example and on image generation benchmarks.
In section \ref{sec:rl}, we proposed f-Restart sampling, a method that is used to correct the sampling error. This method takes inspiration from restart sampling, an experimental method that mixes the advantages of a low discretization error of ODE samplers, and noise correction from SDE samplers. We propose to make this method more dynamic by using a reinforcement learning algorithm to decide when to add noise, and when to perform the denoising step. We perform experiments only on toy datasets, as this method is currently in its early stages of development.
\\
\textbf{Future work :} Our study of discriminator guidance was mostly theoretical, and should be experimented with different pre-trained models, and data types. We proposed in our experiments training EDM to have a fair comparison with the original DG work, but we could have included more experiments to show the flexibility of our method to more models. In order to improve our final loss, other regularization term could be explored that leverage the generative possbilities of the score model. 
\\
The f-Restart sampling method has many improvement potentialities. We believe that restraining the noise addition to the smaller denoising steps could improve the method. Although our weighting scheme was not effective, we believe it may be solved by providing a riforous analysis on the convergence of the method. Furthermore, other reinforcement learning schemes could be proposed, notably imitation learning approaches that learn a reward function given expert trajectories. 
\\
We believe that the scope of this work is important given the current usage of generative models. Boosting methods allow to improve the performance of large pre-trained models with much smaller models, and thus provide a consequential opportunity for efficiency and energy saving. In our work, both discriminator guidance and f restart sampling had the objective of minimizing an f-divergence between the refined distribution and the target distribution. However, other approaches can choose to minimize another objective, such as precision and recall of the generated samples, fairness metrics and bias reduction. We believe that the methods we proposed can be used as a basis for further research in the field of generative models.

